# -*- coding: utf-8 -*-
"""Email spam detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UfUlJWwwD4h2Z7kMt5mga-YORk1Vg-Ve

# **Email spam Detection with Machine Learning**

**Install & Import Required Libraries**
"""

!pip install pandas numpy sklearn matplotlib seaborn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""**Load the Dataset**"""

import pandas  as pd
df = pd.read_csv('/content/spam.csv', encoding='latin-1')
df = df[['v1', 'v2']]
df.columns = ['label', 'message']

df.head()

"""**Features of Dataset**"""

df.columns

df.shape

df.duplicated().sum()

df = df.drop_duplicates(keep='first')

df.isna().sum()

df.shape

df['label'].unique()

df['label'].value_counts()

"""**Label Encoding**"""

from sklearn.preprocessing import LabelEncoder
import pandas as pd
df = pd.DataFrame({'label': ['spam', 'ham', 'spam', 'ham']})

encoder = LabelEncoder()
df['label'] = encoder.fit_transform(df['label'])

df.head()

import matplotlib.pyplot as plt

plt.pie(df['label'].value_counts(), labels=['spam' ,'ham'],autopct='%0.2f')
plt.show()

df = pd.read_csv('/content/spam.csv', encoding='latin-1')
df = df[['v1', 'v2']]
df.columns = ['label', 'message']

df['num_characters'] = df['message'].apply(len)

"""**Download NLTK Resources**"""

import nltk
nltk.download('punkt')

import nltk
nltk.download('punkt_tab')
df['num_word'] = df['message'].apply(lambda x:len(nltk.word_tokenize(x)))

df['num_sentences'] = df['message'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df[['num_characters','num_word','num_sentences']].describe()

df[df['label'] == 'spam'][['num_characters', 'num_word', 'num_sentences']].describe()

df[df['label'] == 'ham'][['num_characters', 'num_word', 'num_sentences']].describe()

"""**Exploratory Data Analysis**"""

import seaborn as sns
sns.histplot(df[df['label'] == 'spam']['num_characters'],color='red')
sns.histplot(df[df['label'] == 'ham']['num_characters'],color='blue')

sns.pairplot(df,hue='label')

sns.heatmap(df[['num_characters','num_word','num_sentences']].corr(),annot=True)

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

"""**Define and Complete transform_text() Function**"""

import nltk
import string
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)

def transform_text(text):
  text = text.lower()
  text = nltk.word_tokenize(text)
  li = list()
  for  word in text:
    if word.isalnum():
      li.append(word)
  text = li.copy()
  li.clear()
  for word in text:
    if word not in stop_words and word not in punctuation:
      li.append(word)
  text = li.copy()
  li.clear()
  for word in text:
    li.append(ps.stem(word))
  text=li.copy()
  return " ".join(text)

"""**Apply the Function to Dataset**"""

df['transformed_text'] = df['message'].apply(transform_text)
print(df['transformed_text'])

df

from wordcloud import WordCloud

spam_wc = WordCloud(width=150,height=100,background_color='white')
spam_wc.generate(df[df['label'] == 'spam']['transformed_text'].str.cat(sep=" "))

import matplotlib.pyplot as plt
plt.imshow(spam_wc)
plt.axis('off')
plt.show()

"""**Feature Extraction (Text Vectorization)**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=3000)
X = tfidf.fit_transform(df['transformed_text']).toarray()

y = df['label'].values

"""**Train-Test Split**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Model Training**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

model = MultinomialNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

"""**Model Evaluation**"""

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""**Predict on Custom Input**"""

import nltk
import string
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)
ps = PorterStemmer()

def transform_text(text):
    text = text.lower()
    words = nltk.word_tokenize(text)
    filtered = [word for word in words if word.isalnum()]
    filtered = [word for word in filtered if word not in stop_words]
    stemmed = [ps.stem(word) for word in filtered]
    return " ".join(stemmed)

import pandas as pd


df = pd.read_csv('/content/spam.csv', encoding='latin-1')[['v1', 'v2']]
df.columns = ['label', 'message']
df['label'] = df['label'].map({'ham': 0, 'spam': 1})
df['transformed_message'] = df['message'].apply(transform_text)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=3000)
X = tfidf.fit_transform(df['transformed_message']).toarray()
y = df['label'].values

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = MultinomialNB()
model.fit(X_train, y_train)

def predict_spam(text):
    transformed = transform_text(text)
    vector = tfidf.transform([transformed]).toarray()
    result = model.predict(vector)[0]
    return "Spam" if result == 1 else "Not Spam"


print(predict_spam("Congratulations! You've won a free vacation. Click to claim now!"))